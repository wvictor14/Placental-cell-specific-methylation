---
title: "1_7_classify_methylation_states"
author: "Victor Yuan"
date: "04/06/2019"
output:
  html_document:
    keep_md: true
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
    theme: spacelab
editor_options: 
  chunk_output_type: console
---

For implementing mixture modelling to classify methylated, intermediate and unmethylated states.


# Setup

## Libraries

```{r}
library(minfi)
library(tidyverse)
library(mixtools)
library(betareg)
library(ewastools)
library(purrr)
```

## Real Data

```{r}
pDat <- readRDS('../../data/main/interim/1_3_pDat.rds')
et_meth <- readRDS('../../data/main/interim/0_1_ewastools_loaded_idats.rds')
snps <- et_meth$manifest[probe_type=="rs",index]
et_betas <- dont_normalize(et_meth)
snps <- et_betas[snps,]


testdata <- snps[,1]
```

## test data

compare to ewastools

```{r}
#ewastools pipeline
snps_called <- call_genotypes(testdata, learn = T)
mxm_(snps_called)
snps_called$par
```

$pi
[1] 0.3146189 0.4458035 0.2395776

$shapes1
[1]   5.06776  30.45717 137.50934

$shapes2
[1] 139.09278  41.33075   7.94062

$alpha
[1] 0.04319594

# 3 component beta

```{r eval = F}
# Expectation
# obtain posteriors
e_step1 <- function(data, shape1, shape2, alpha){
  
  input_data <- tibble(data = data)
  
  # input parameter dataframe
  calculations <- tibble(k = as.character(1:3),
         shape1 = shape1,
         shape2 = shape2,
         alpha = alpha) %>%
    
    # calculate numerator
    mutate(px_given_kj = map2(shape1, shape2, ~dbeta(x = input_data$data,
                                                     shape1 = .x,
                                                     shape2 = .y))) %>%
  
    mutate(px_given_kj_alpha = map2(px_given_kj, alpha, ~ .x * .y)) 
  
  # calculate denominator
  input_data <- input_data %>%
    mutate(prob_x = purrr::reduce(calculations$px_given_kj_alpha, `+`))
  
  # solve for posterior
  calculations <-  calculations %>%
    mutate(posterior = map(px_given_kj_alpha, ~ .x / input_data$prob_x))
  
  # solve for log likelihood
  log_likelihood <- sum(log(input_data$prob_x))
  
  return(list('Log_likelihood' = log_likelihood,
              'Posterior' = calculations %>% select(k, posterior)))
}

# maximization
# update parameters
m_step1 <- function(data, posterior_df){
  # calculate mean (mu) and variance (var) using posteriors
  # number of samples in each class, and overall
  
  n_all <- length(data) # number of observations
  
  parameters <- posterior_df %>%
    
    # weighted n's by their posterior probs
    mutate(n = map_dbl(posterior, sum)) %>%  
    
    # mean: sum over x weighted by posterior probs
    mutate(mu = map2_dbl(posterior, n, ~ (1/.y) * sum(data * .x))) %>% 
    
    # variance 
    # (mean(w*x^2)-sample.mean^2) * n/(n-1)
    mutate(var = map2_dbl(n, posterior, ~ mean(data^2 * .y* n_all/ .x))) %>%
    mutate(var =  (var-mu^2) * n_all/(n_all-1)) %>%
    
    
    # shape1 shape2 (alpha,beta)
    mutate(shape1 = mu * (((mu * (1 - mu)) / var)-1),
           shape2 = (1 - mu) * (((mu * (1 - mu)) / var) - 1)) %>%
    
    # alpha
    mutate(alpha = n/sum(n))
  
  return(parameters)
}

# put it all into a loop:
call_meth_state <- function(x, converge_n_iter = 20,  converge_diff = 1e-6, k = 3){
  
  # x is a vector of the data
  # converge_n_iter is the number of maximum EM iterations
  # converge_diff is the minimum difference in log likelihood between iterations before stopping
  
  # 1. Initial parameters
  shape1 <- c(10, 80, 80)
  shape2 <- c(80, 80, 10)
  alpha <- c(1/3, 1/3, 1/3) # mixing proportions
  
  # 2. expectation
  e_step_out <- e_step1(data = x, shape1, shape2, alpha)
  
  # 3. maximization
  updated_parameters <- m_step1(data = x, posterior_df = e_step_out$Posterior)
  
  Log_likelihood <- e_step_out$Log_likelihood

  # Repeat 2. and 3. until convergence
  for (i in 1:converge_n_iter) {
    e_step_out <- e_step1(data = x, 
                         shape1 = updated_parameters$shape1, 
                         shape2 = updated_parameters$shape2,
                         alpha = updated_parameters$alpha)
    updated_parameters <- m_step1(data = x, posterior_df = e_step_out$Posterior)
    
    Log_likelihood <- c(Log_likelihood, e_step_out$Log_likelihood)
    
    if (abs(Log_likelihood[i+1]-Log_likelihood[i]) < converge_diff){
      break
    }    
  }
  
  out <- list()
  out[['Parameters']] <- updated_parameters
  out[['Posterior']] <- e_step_out$Posteriors
  out[['Log_likelihood']] <- Log_likelihood
  
  return(out)

}
```

Try it out

```{r}
snps_called_pl <- call_meth_state(x = testdata)
snps_called_pl

plot_meth_states <- function(called_states, data = x) {
  
  # generate density line
  density <- called_states$Parameters %>% 
    crossing(x = seq(0.01, 1, 0.01)) %>% 
    mutate(y = alpha*dbeta(x = x, shape1 = shape1, shape2 = shape2),
           k = as.character(k))
  
  ggplot(data.frame(x = data), aes(x = x)) +
    geom_histogram(binwidth = 0.01, aes(y = ..density..)) +
    geom_density(col = 'blue', linetype = 'dashed', size = 1) +
    geom_line(data = density, aes(x = x, y = y, color = k), size = 1)
}
plot_meth_states(snps_called_pl)
```

# ewastools

```{r}
eBeta = function(x,w){
	
	n = length(w)
	#nw <- n/sum(w)
	
	w = n*w/sum(w)
	sample.mean =  mean(w*x)
    sample.var  = (mean(w*x^2)-sample.mean^2) * n/(n-1)
    v = sample.mean * (1-sample.mean)
    
    if (sample.var < v){
        shape1 = sample.mean * (v/sample.var - 1)
        shape2 = (1 - sample.mean) * (v/sample.var - 1)
    } else {
        shape2 = sample.mean * (v/sample.var - 1)
        shape1 = (1 - sample.mean) * (v/sample.var - 1)
    }
    
    list(shape1 = shape1, shape2 = shape2)
}

pi = c(1/3,1/3,1/3) # Class probabilities
shapes1 = c(10,80,80) 
shapes2 = c(80,80,10)
gamma = NA

e_step = function(){
	gamma = cbind(
		 pi[1] * dbeta(testdata,shape1=shapes1[1],shape2=shapes2[1])
		,pi[2] * dbeta(testdata,shape1=shapes1[2],shape2=shapes2[2])
		,pi[3] * dbeta(testdata,shape1=shapes1[3],shape2=shapes2[3])
		)

	tmp = rowSums(gamma)
	gamma <- gamma/tmp

	loglik = tmp
	loglik = sum(log(loglik))

	return(loglik)
}

m_step = function(){

	# MLE
	s1 = eBeta(testdata,gamma[,1])
	s2 = eBeta(testdata,gamma[,2])
	s3 = eBeta(testdata,gamma[,3])

	shapes1 <<- c(s1$shape1,s2$shape1,s3$shape1)
	shapes2 <<- c(s1$shape2,s2$shape2,s3$shape2)

	# MLE of class priors
	pi = apply(gamma,2,sum)
	pi <- pi/sum(pi)
	
	invisible(NULL)
}

loglik = rep(NA_real_,5)
loglik[1] = e_step()

i = 2; gain=Inf;

while(i<5 & gain>1e-4){
	m_step()
	loglik[i] = e_step()
	gain = loglik[i]-loglik[i-1]
	i=i+1
}

loglik=loglik[1:(i-1)]


```


# Step by step
# 2 component beta

## Initialization

Specify starting parameters for the two beta distributions, we can start by specifying the mean and sd,
then calculate the two shape parameters for the beta distribution. The standard nomenclature for the
two beta distributions parameters is 'beta' and 'alpha'. To avoid confusion, I call beta shape2 and alpha shape1

\[
mu = 0.25, 0.75\\
sigma = 0.1, 0.1\\
alpha = 0.5, 0.5
\]

\[
\mu = \frac{\alpha}{\alpha + \beta} = \frac{shape1}{shape1 + shape2}\\
\sigma^2 = \frac{\mu(1-\mu)}{\alpha + \beta + 1} =\frac{\mu(1-\mu)}{shape1 + shape2 + 1} 
\]

Scratch that, instead of doing math, I will just specify the shape parameters

```{r}
#shape1 and shape2
shape1 <- c(10, 80)
shape2 <- c(80, 10)

# mixing proportions
alpha <- c(0.5, 0.5)
```

## Expectation

Produce soft labels for each data point, i.e. calculate the posterior probability each sample
belongs to component 1 and component 2

\[
P(x_i \in k_j | x_i) = \frac{P(x_i|x_i \in k_j)P(k_j)}{P(x_i)}
\]


where,
$P(x_i|x_i \in k_j)$ is the probability of the data given it comes from component j, and can be 
calculated using `stats::dbeta`.

$P(k_i)$ is the mixing proportion for component j.

The denominator $P(x_i)$ can be calculated by summing the numerators over all components j.

```{r}
numerator_k1 <- dbeta(x = testdat, shape1 = shape1[1], shape2 = shape2[1])  * alpha[1]
numerator_k2 <- dbeta(x = testdat, shape1 = shape1[2], shape2 = shape2[2])  * alpha[2]

prob_x <- numerator_k1 + numerator_k2

posterior_k1 <- numerator_k1 / prob_x
posterior_k2 <- numerator_k2 / prob_x
```

Also calculate the starting log odds. Will need to save this for each iteration

```{r}
log_odds <- sum(log(base = exp(1), x = prob_x)) 
log_odds
```

## Maximization

Now that we have posterior probabilities, we can update the parameters of each distribution. We can
rely on mle to estimate shape1 and shape2. Alpha is calculated by the sum over posterior probabilities.

Forbes et al., 2011 gives the method of moments estimators of the shape parameters:

$$\begin{aligned}
shape1 = \mu\left(\frac{\mu(1-\mu)}{\sigma^2}-1\right)\\
shape2 = (1-\mu)\left(\frac{\mu(1-\mu)}{\sigma^2}-1\right)\\
\end{aligned}$$

where the mean $\mu$ and variance $\sigma$ can be calculated by:

\[
\begin{align}
\mu = \frac{1}{n}\sum_{i=1}^{n}{x_i}\\
\sigma^2 = E[(X - \mu)^2] 
         = \frac{1}{n}\sum_{i=1}^{n}{x_i^2 - (\frac{1}{n}\sum_{i=1}^{n}{x_i)^2})}\\
\end{align}
\]

The variance can also be calculated by:

\[
\begin{align}
\sigma^2 = E[X^2] - (E[X])^2\\ 
         = \frac{1}{n}\sum_{i=1}^{n}{(x_i-\mu)^2}\\
\end{align}
\]

although the variance estimator can be replaced with an unbiased estimator of variance:

\[
\sigma^2 = \frac{1}{n-1}\sum_{i=1}^{n}{(x_i-\mu)^2}
\]

For us because we have multiple components we need to weight the observations by their posterior 
probabilities that they belong to that component. So, let's say $\mu_j$ is our estimated mean for
component j, this is

\[
\begin{align}
\mu_j = \frac{1}{n_j}\sum_{i=1}^{n}{x_iP(x_i \in k_j|x_i)}\\
n_j = \sum_{i=1}^{n}{P(x_i \in k_j|x_i)}
\end{align}
\]

$n_j$ is derived from summing all of the probabilities for component $j$

A similar formula for variance is derived:

\[\begin{align}
\sigma_j^2 = E[X_j^2] - (E[X_j])^2 \\
= \frac{1}{n_j}\left(\sum_{i=1}^{n}{P(x_i \in k_j|x_i)x_i^2} - \mu_j^2\right) \\
\end{align}
\]

For an unbiased estimator we can multipy by $\frac{1}{n-1}$ instead of $\frac{1}{n}$:

\[
\sigma_j^2 = \frac{1}{n_j-1}\left(\sum_{i=1}^{n}{P(x_i \in k_j|x_i)x_i^2} - \mu_j^2\right) \\
\]

Calculat $\mu$ and $\sigma^2$
```{r}
n <- length(posterior_k1)
n1 <- sum(posterior_k1) # number samples in each class
n2 <- sum(posterior_k2)

mu_1 <- 1/n1 * sum(testdat*posterior_k1)
mu_2 <- 1/n2 * sum(testdat*posterior_k2)
mu <- c(mu_1, mu_2)

var_1 <- ((1/(n1-1)) * sum(posterior_k1 *testdat^2) - mu_1^2)
var_2 <- ((1/(n2-1)) * sum(posterior_k2 *testdat^2) - mu_2^2)
var <- c(var_1, var_2)

parameters <- data.frame(
  component = c('1', '2'),
  mu = c(mu_1, mu_2),
  var = c(var_1, var_2)
)
```

Solve for shape1 and shape2

```{r}
parameters <- parameters %>%
  mutate(shape1 = mu * (((mu * (1 - mu)) / var)-1),
         shape2 = (1 - mu) * (((mu * (1 - mu)) / var) - 1))
```




