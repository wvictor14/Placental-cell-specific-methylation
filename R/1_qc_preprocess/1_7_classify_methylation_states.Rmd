---
title: "1_7_classify_methylation_states"
author: "Victor Yuan"
date: "04/06/2019"
output:
  html_document:
    keep_md: true
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
    theme: spacelab
editor_options: 
  chunk_output_type: console
---

For implementing mixture modelling to classify methylated, intermediate and unmethylated states.


# Setup

## Libraries

```{r}
library(minfi)
library(tidyverse)
library(mixtools)
library(betareg)
library(ewastools)
library(purrr)
```

## Real Data

```{r}
pDat <- readRDS('../../data/main/interim/1_3_pDat.rds')
betas <- readRDS('../../data/main/interim/1_4_betas_noob_filt.rds')
et_meth <- readRDS('../../data/main/interim/0_1_ewastools_loaded_idats.rds')
dim(betas)
dim(pDat)
all(pDat$Sentrix == colnames(betas))
colnames(betas) <- pDat$Sample_Name 
```

# 2 component gamma

Model components:
1. Gamma
2. Gamma

Input:
betas

```{r}
# subset to 1000
testdat <- sample(betas, size = 10000)

# plot empirical 
qplot(x = testdat) + geom_histogram()

fit_gamma <- gammamixEM(testdat,
                        lambda = c(0.5, 0.5),# initial mixing proportions
                        verb = T) 

fit_gamma$all.loglik

plot(fit_gamma)

densities <- tibble(quantiles = seq(0,1,0.01)) %>%
  mutate(component1 = fit_gamma$lambda[1]*dgamma(quantiles,
                                                 shape =  fit_gamma$gamma.pars[1,1],
                                                 scale = fit_gamma$gamma.pars[2,1]),
         component2 = fit_gamma$lambda[2]*dgamma(quantiles,
                                                 shape =  fit_gamma$gamma.pars[1,2],
                                                 scale = fit_gamma$gamma.pars[2,2]))
tibble(data = testdat) %>%
  ggplot(aes(x = data)) + 
  geom_histogram(aes(y = ..density..)) +
  geom_density() +
  geom_line(data = densities, aes(x = quantiles, y =component1), color = 'red') +
  geom_line(data = densities, aes(x = quantiles, y = component2), color = 'blue')
```


# 2 component normals

```{r}
mvals <- logit2(betas)

mvals_test <- sample(mvals, size = 1000000)


data.frame(mvals = mvals_test) %>%
  ggplot(aes(x = mvals)) +
  geom_histogram(aes(y = ..density..)) +
  geom_density()
```


# 2 component beta

## Initialization

Specify starting parameters for the two beta distributions, we can start by specifying the mean and sd,
then calculate the two shape parameters for the beta distribution. The standard nomenclature for the
two beta distributions parameters is 'beta' and 'alpha'. To avoid confusion, I call beta shape2 and alpha shape1

\[
mu = 0.25, 0.75\\
sigma = 0.1, 0.1\\
alpha = 0.5, 0.5
\]

\[
\mu = \frac{\alpha}{\alpha + \beta} = \frac{shape1}{shape1 + shape2}\\
\sigma^2 = \frac{\mu(1-\mu)}{\alpha + \beta + 1} =\frac{\mu(1-\mu)}{shape1 + shape2 + 1} 
\]

Scratch that, instead of doing math, I will just specify the shape parameters

```{r}
#shape1 and shape2
shape1 <- c(10, 80)
shape2 <- c(80, 10)

# mixing proportions
alpha <- c(0.5, 0.5)
```

## Expectation

Produce soft labels for each data point, i.e. calculate the posterior probability each sample
belongs to component 1 and component 2

\[
P(x_i \in k_j | x_i) = \frac{P(x_i|x_i \in k_j)P(k_j)}{P(x_i)}
\]


where,
$P(x_i|x_i \in k_j)$ is the probability of the data given it comes from component j, and can be 
calculated using `stats::dbeta`.

$P(k_i)$ is the mixing proportion for component j.

The denominator $P(x_i)$ can be calculated by summing the numerators over all components j.

```{r}
numerator_k1 <- dbeta(x = testdat, shape1 = shape1[1], shape2 = shape2[1])  * alpha[1]
numerator_k2 <- dbeta(x = testdat, shape1 = shape1[2], shape2 = shape2[2])  * alpha[2]

prob_x <- numerator_k1 + numerator_k2

posterior_k1 <- numerator_k1 / prob_x
posterior_k2 <- numerator_k2 / prob_x
```

Also calculate the starting log odds. Will need to save this for each iteration

```{r}
log_odds <- sum(log(base = exp(1), x = prob_x)) 
log_odds
```

## Maximization

Now that we have posterior probabilities, we can update the parameters of each distribution. We can
rely on mle to estimate shape1 and shape2. Alpha is calculated by the sum over posterior probabilities.

Forbes et al., 2011 gives the method of moments estimators of the shape parameters:

$$\begin{aligned}
shape1 = \mu\left(\frac{\mu(1-\mu)}{\sigma^2}-1\right)\\
shape2 = (1-\mu)\left(\frac{\mu(1-\mu)}{\sigma^2}-1\right)\\
\end{aligned}$$

where the mean $\mu$ and variance $\sigma$ can be calculated by:

\[
\begin{align}
\mu = \frac{1}{n}\sum_{i=1}^{n}{x_i}\\
\sigma^2 = E[(X - \mu)^2] 
         = \frac{1}{n}\sum_{i=1}^{n}{x_i^2 - (\frac{1}{n}\sum_{i=1}^{n}{x_i)^2})}\\
\end{align}
\]

The variance can also be calculated by:

\[
\begin{align}
\sigma^2 = E[X^2] - (E[X])^2\\ 
         = \frac{1}{n}\sum_{i=1}^{n}{(x_i-\mu)^2}\\
\end{align}
\]

although the variance estimator can be replaced with an unbiased estimator of variance:

\[
\sigma^2 = \frac{1}{n-1}\sum_{i=1}^{n}{(x_i-\mu)^2}
\]

For us because we have multiple components we need to weight the observations by their posterior 
probabilities that they belong to that component. So, let's say $\mu_j$ is our estimated mean for
component j, this is

\[
\begin{align}
\mu_j = \frac{1}{n_j}\sum_{i=1}^{n}{x_iP(x_i \in k_j|x_i)}\\
n_j = \sum_{i=1}^{n}{P(x_i \in k_j|x_i)}
\end{align}
\]

$n_j$ is derived from summing all of the probabilities for component $j$

A similar formula for variance is derived:

\[\begin{align}
\sigma_j^2 = E[X_j^2] - (E[X_j])^2 \\
= \frac{1}{n_j}\left(\sum_{i=1}^{n}{P(x_i \in k_j|x_i)x_i^2} - \mu_j^2\right) \\
\end{align}
\]

For an unbiased estimator we can multipy by $\frac{1}{n-1}$ instead of $\frac{1}{n}$:

\[
\sigma_j^2 = \frac{1}{n_j-1}\left(\sum_{i=1}^{n}{P(x_i \in k_j|x_i)x_i^2} - \mu_j^2\right) \\
\]

Calculat $\mu$ and $\sigma^2$
```{r}
n <- length(posterior_k1)
n1 <- sum(posterior_k1) # number samples in each class
n2 <- sum(posterior_k2)

mu_1 <- 1/n1 * sum(testdat*posterior_k1)
mu_2 <- 1/n2 * sum(testdat*posterior_k2)
mu <- c(mu_1, mu_2)

var_1 <- ((1/(n1-1)) * sum(posterior_k1 *testdat^2) - mu_1^2)
var_2 <- ((1/(n2-1)) * sum(posterior_k2 *testdat^2) - mu_2^2)
var <- c(var_1, var_2)

parameters <- data.frame(
  component = c('1', '2'),
  mu = c(mu_1, mu_2),
  var = c(var_1, var_2)
)
```

Solve for shape1 and shape2

```{r}
parameters <- parameters %>%
  mutate(shape1 = mu * (((mu * (1 - mu)) / var)-1),
         shape2 = (1 - mu) * (((mu * (1 - mu)) / var) - 1))
```

## put it all together

wrap it in a function

```{r eval = F}
#shape1 and shape2
shape1 <- c(10, 80)
shape2 <- c(80, 10)

# mixing proportions
alpha <- c(0.5, 0.5)

# Expectation
# obtain posteriors
e_step <- function(data, shape1, shape2, alpha, k){
  
  # input parameter dataframe
  calculations <- tibble(k = 1:length(shape1),
         shape1 = shape1,
         shape2 = shape2,
         alpha = alpha) %>%
    
    # calculate numerator
    mutate(px_given_kj = lapply(k, function(x) dbeta(data, shape1 = shape1[x], 
                                                           shape2 = shape2[x]))) %>%
    mutate(px_given_kj_alpha = purrr::map(px_given_kj, ~ .x * alpha)) 
  
  # calculate denominator
  prob_x <- calculations %>% 
    pull(px_given_kj_alpha) %>%
    purrr::reduce(`+`)
  
  # solve for posterior
  calculations <-  calculations %>%
    mutate(posterior = purrr::map(px_given_kj_alpha, ~ .x / prob_x))
  
  # solve for log likelihood
  log_likelihood <- sum(log(prob_x))
  
  return(list('Log_likelihood' = log_likelihood,
              'Posterior' = calculations %>% select(k, posterior)))
  
  prob_x_given_from_k1 <- dbeta(x = data, shape1 = shape1[1], shape2 = shape2[1])
  prob_x_given_from_k2 <- dbeta(x = data, shape1 = shape1[2], shape2 = shape2[2])
  
  prob_x <- (prob_x_given_from_k1 * alpha[1]) + (prob_x_given_from_k2 * alpha[2])
  
  posterior_k1 <- (prob_x_given_from_k1 * alpha[1]) / prob_x
  posterior_k2 <- (prob_x_given_from_k2 * alpha[2]) / prob_x
  
  # return log likelihood and posterior probs
  out <- list()
  out[['Log_likelihood']] <- sum(log(prob_x))
  out[['Posteriors']] <- tibble(k1 = posterior_k1,
                                k2 = posterior_k2)
  
  return(out)
}



# maximization
# update parameters
m_step <- function(data, p1, p2){
  # calculate mean (mu) and variance (var) using posteriors
  
  # number of samples in each class, and overall
  n <- length(p1)
  n1 <- sum(p1) 
  n2 <- sum(p2) # n1+n2 = n
  
  mu_1 <- 1/n1 * sum(data*p1)
  mu_2 <- 1/n2 * sum(data*p2)
  mu <- c(mu_1, mu_2)
  
  var_1 <- ((1/(n1-1)) * sum(p1 * data^2) - mu_1^2)
  var_2 <- ((1/(n2-1)) * sum(p2 * data^2) - mu_2^2)
  var <- c(var_1, var_2)
  
  parameters <- tibble(
    component = c('1', '2'),
    alpha = c(mean(p1), mean(p2)), 
    mu = c(mu_1, mu_2),
    var = c(var_1, var_2)
  ) %>%
    
    # calculate shape1 shape2 based on definition of beta distribution 
    dplyr::mutate(shape1 = mu * (((mu * (1 - mu)) / var)-1),
                  shape2 = (1 - mu) * (((mu * (1 - mu)) / var) - 1))
  
  return(parameters)
}

# put it all into a loop:

call_meth_state <- function(x, converge_n_iter = 50,  converge_diff = 1e-6) {
  
  # x is a vector of the data
  # converge_n_iter is the number of maximum EM iterations
  # converge_diff is the minimum difference in log likelihood between iterations before stopping
  
  # 1. Initial parameters
  shape1 <- c(10, 80)
  shape2 <- c(80, 10)

  # mixing proportions
  alpha <- c(0.5, 0.5)
  
  # 2. expectation
  e_step_out <- e_step(data = x, shape1, shape2, alpha)
  
  # 3. maximization
  updated_parameters <- m_step(data = x, 
       p1 = e_step_out$Posteriors$k1, 
       p2 = e_step_out$Posteriors$k2)
  
  Log_likelihood <- e_step_out$Log_likelihood

  # Repeat 2. and 3. until convergence
  for (i in 1:converge_n_iter) {
    e_step_out <- e_step(data = x, 
                         shape1 = updated_parameters$shape1, 
                         shape2 = updated_parameters$shape2,
                         alpha = updated_parameters$alpha)
    updated_parameters <- m_step(data = x,
                                 p1 = e_step_out$Posteriors$k1,
                                 p2 = e_step_out$Posteriors$k2)
    
    Log_likelihood <- c(Log_likelihood, e_step_out$Log_likelihood)
    
    if (abs(Log_likelihood[i+1]-Log_likelihood[i]) < converge_diff){
      break
    }    
  }
  
  out <- list()
  out[['Parameters']] <- updated_parameters
  out[['Posterior']] <- e_step_out$Posteriors
  out[['Log_likelihood']] <- Log_likelihood
  
  return(out)

}
```

Test on 1000 cpgs

```{r}
testdata <- sample(betas, size = 1000)
results_1000 <- call_meth_state(x = testdata, converge_n_iter = 50)
results_1000

# generate density lines for plotting
p_density <- results_1000$Parameters %>% 
  select(component, alpha, shape1, shape2) %>%
   # 1. Initial parameters
  rbind(tibble(
    component = c('1_i', '2_i'),
    alpha = c(0.5, 0.5),
    shape1 = c(10, 80),
    shape2 = c(80, 10))) %>%
  # compute density over quantiles x
  crossing(x = seq(0.01, 1, 0.01)) %>% 
  mutate(y = dbeta(x = x, shape1 = shape1, shape2 = shape2))
                              
pp1 <- ggplot(data.frame(x = testdata), aes(x = x)) +
  geom_histogram(binwidth = 0.02, aes(y = ..density..))  +
  geom_density(col = 'blue', linetype = 'dashed', size = 1) +
  geom_line(data = p_density, aes(x = x, y = y, color = component), size = 2) +
  theme_bw() +
  labs(title = '1000 cpgs');pp1
```


```{r}
testdata <- sample(betas, size = 10000)
results_10000 <- call_meth_state(x = testdata, converge_n_iter = 50)
results_10000


# generate density lines for plotting
p_density <- results_10000$Parameters %>% crossing(x = seq(0.01, 1, 0.01)) %>% 
  mutate(y = dbeta(x = x, shape1 = shape1, shape2 = shape2))
                              
pp2 <- ggplot(data.frame(x = testdata), aes(x = x)) +
  geom_histogram(binwidth = 0.02, aes(y = ..density..))  +
  geom_density(col = 'blue', linetype = 'dashed', size = 1) +
  geom_line(data = p_density, aes(x = x, y = y, color = component), size = 2) +
  theme_bw() +
  labs(title = '10000 cpgs')
```

```{r}
testdata <- sample(betas, size = 100000)
results_100000 <- call_meth_state(x = testdata, converge_n_iter = 50)
results_100000


# generate density lines for plotting
p_density <- results_100000$Parameters %>% crossing(x = seq(0.01, 1, 0.01)) %>% 
  mutate(y = dbeta(x = x, shape1 = shape1, shape2 = shape2))
                              
pp3 <- ggplot(data.frame(x = testdata), aes(x = x)) +
  geom_histogram(binwidth = 0.02, aes(y = ..density..))  +
  geom_density(col = 'blue', linetype = 'dashed', size = 1) +
  geom_line(data = p_density, aes(x = x, y = y, color = component), size = 2) +
  theme_bw() +
  labs(title = '100000 cpgs')
```

Test it again for 1 sample

```{r}
results_1samp <- call_meth_state(x = betas[,1], converge_n_iter = 50)
results_1samp


# generate density lines for plotting
p_density_1samp <- results_1samp$Parameters %>% crossing(x = seq(0.01, 1, 0.01)) %>% 
  mutate(y = dbeta(x = x, shape1 = shape1, shape2 = shape2))
                              
pp4 <- ggplot(data.frame(x = betas[,1]), aes(x = x)) +
  geom_histogram(binwidth = 0.02, aes(y = ..density..))  +
  geom_density(col = 'blue', linetype = 'dashed', size = 1) +
  geom_line(data = p_density_1samp, aes(x = x, y = y, color = component), size = 2) +
  theme_bw() +
  labs(title = '700 000 cpgs')
```




```{r}
cowplot::plot_grid(pp1, pp2, pp3, pp4)
```


# 3 component beta

Let me try a 3 component beta distribution and compare to ewastools

```{r}
#ewastools pipeline
snps <- et_meth$manifest[probe_type=="rs",index]
et_betas <- dont_normalize(et_meth)
snps <- et_betas[snps,]

# fit mixture model to call genotypes
snps_called <- call_genotypes(snps, learn = T)
```







